# -*- coding: utf-8 -*-
"""Dataset preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlscfTSgGTe0spTmQs3t_XXQhjsU_7NJ

# 0- Setup
"""

import os
import pandas as pd

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
dataset_path = os.path.join(BASE_DIR, "dataset.xlsx")

OUTPUT_ROOT = os.path.join(BASE_DIR, "outputs", "Dataset preprocessing")
OUTLIER_FIG_PATH = os.path.join(OUTPUT_ROOT, "Outlier figures")
FEATURE_CORR_PATH = os.path.join(OUTPUT_ROOT, "Feature correlation")
FEATURE_SELECTION_PATH = os.path.join(OUTPUT_ROOT, "Feature selection")
CLUSTERING_PATH = os.path.join(OUTPUT_ROOT, "Clustering")

for directory in [
    OUTPUT_ROOT,
    OUTLIER_FIG_PATH,
    FEATURE_CORR_PATH,
    FEATURE_SELECTION_PATH,
    CLUSTERING_PATH,
]:
  os.makedirs(directory, exist_ok=True)

data = pd.read_excel(dataset_path)
print(data.shape)
data.head()

# © Sadegh Tale

# packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['svg.fonttype'] = 'none'

import seaborn as sns
sns.set()
sns.set_theme(style="white")
sns.set_style("ticks")
sns.color_palette("husl", 9)


import sklearn
from matplotlib.patches import Circle
from matplotlib.legend_handler import HandlerPathCollection
from sklearn.impute import SimpleImputer

import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# folder paths
random_state = 0

def save_plot(figure, file_name, directory):
  os.makedirs(directory, exist_ok=True)
  file_path = os.path.join(directory, f"{file_name}.svg")
  figure.savefig(file_path, format='svg')

def load_data(training_path, train_ratio, test_ratio, val_ratio):
  # Load data
  df = pd.read_excel(training_path)
  df.pop("No") #Remove later
  # Data filling
  nan_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
  df = pd.DataFrame(nan_imputer.fit_transform(df), columns=df.columns)
  # Data shuffle
  df = df.sample(frac=1, random_state=0)
  df.reset_index(drop=True, inplace=True)
  # Accounting for the concentration in IE
  df["IE"] = (df["IE"] * df["AA"])/100
  df.drop(columns=["AA"], inplace=True)

  # Data split
  train_split_index = int(len(df) * train_ratio)
  test_split_index = train_split_index + int(len(df) * test_ratio)
  # val_split_index = train_split_index + int(len(df) * val_ratio)

  train_df = df[:train_split_index]
  test_df = df[train_split_index:test_split_index:]
  val_df = df[test_split_index:]
  # test_df = df[val_split_index:]

  return train_df, test_df, val_df, train_df.columns
  # # Features: df.iloc[:, 0:6]
  # # Output:   df.iloc[:, 6]



dataset, _, _, _ = load_data(training_path=dataset_path, train_ratio=1, test_ratio=0, val_ratio=0)
dataset

"""# 1- Outlier detection

## 3D plot of feature outliers
"""

from sklearn.decomposition import PCA
from sklearn.covariance import EllipticEnvelope
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from matplotlib.legend_handler import HandlerPathCollection
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Columns to examine:
X = dataset.iloc[:, 0:6].copy()
y = dataset.iloc[:, 6].copy()

# Define the features subset to use
features = X.columns[0:6]

# Apply PCA with 3 components
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X[features])
X_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])

# PCA range for axis extremes
pc1_min, pc1_max = X_pca['PC1'].min(), X_pca['PC1'].max()
pc2_min, pc2_max = X_pca['PC2'].min(), X_pca['PC2'].max()
pc3_min, pc3_max = X_pca['PC3'].min(), X_pca['PC3'].max()
print(f"PC1 range: {pc1_min:.2f} → {pc1_max:.2f}")
print(f"PC2 range: {pc2_min:.2f} → {pc2_max:.2f}")
print(f"PC3 range: {pc3_min:.2f} → {pc3_max:.2f}")

# Fit each outlier detector
elliptic_envelope = EllipticEnvelope(contamination=0.1).fit(X[features])
X['EllipticEnvelope']      = elliptic_envelope.predict(X[features])
X['EllipticEnvelope_Score'] = elliptic_envelope.mahalanobis(X[features])

isolation_forest = IsolationForest(contamination=0.1, random_state=0).fit(X[features])
X['IsolationForest']      = isolation_forest.predict(X[features])
X['IsolationForest_Score'] = -isolation_forest.decision_function(X[features])

lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
X['LOF']       = lof.fit_predict(X[features])
X['LOF_Score'] = -lof.negative_outlier_factor_

one_class_svm = OneClassSVM(nu=0.1).fit(X[features])
X['OneClassSVM']      = one_class_svm.predict(X[features])
X['OneClassSVM_Score'] = -one_class_svm.decision_function(X[features])

# Count outliers
n_errors = {
    'EllipticEnvelope': (X['EllipticEnvelope'] == -1).sum(),
    'IsolationForest':  (X['IsolationForest']  == -1).sum(),
    'LOF':              (X['LOF']              == -1).sum(),
    'OneClassSVM':      (X['OneClassSVM']      == -1).sum()
}

def normalize(series):
    return (series - series.min()) / (series.max() - series.min())

# Set up 3D plot
fig = plt.figure(figsize=(8, 8))

ax  = fig.add_subplot(111, projection='3d')

# Plot inliers
ax.scatter(X_pca['PC1'], X_pca['PC2'], X_pca['PC3'],
           color="black", s=10, label="Data points", marker="o")

# Helper to plot each method’s outliers
def plot_outliers(method, color):
    mask   = X[method] == -1
    sizes  = 1000 * normalize(X[f"{method}_Score"][mask])
    ax.scatter(X_pca.loc[mask, 'PC1'],
               X_pca.loc[mask, 'PC2'],
               X_pca.loc[mask, 'PC3'],
               color=color, s=sizes,
               alpha=0.5, edgecolors=color,
               label=f"{method} Outliers ({n_errors[method]})")

# Plot each
plot_outliers('EllipticEnvelope', 'red')
plot_outliers('IsolationForest',  'lime')
plot_outliers('LOF',              'orange')
plot_outliers('OneClassSVM',      'purple')

# Legend handler to fix marker sizes
class LegendHandlerPathCollection(HandlerPathCollection):
    def create_artists(self, legend, orig_handle,
                       xdescent, ydescent, width, height, fontsize, trans):
        leglines = super().create_artists(
            legend, orig_handle, xdescent, ydescent, width, height, fontsize, trans)
        for legline in leglines:
            legline.set_sizes([20])
        return leglines

ax.legend(loc='upper left', fontsize="small",
          handler_map={col: LegendHandlerPathCollection()
                       for col in ax.collections})

# Labels and title
ax.set_title("Outlier Detection Comparison - PCA")
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.set_zlabel("Principal Component 3")

# Optionally tighten axes limits a bit
for axis, pc in zip([ax.set_xlim, ax.set_ylim, ax.set_zlim],
                    ['PC1', 'PC2', 'PC3']):
    vals = X_pca[pc]
    mn, mx = vals.min(), vals.max()
    span = mx - mn
    axis(mn - 0.1*span, mx + 0.1*span)

ax.view_init(elev=30, azim=-37.5)

# Save and show
save_path = OUTLIER_FIG_PATH
save_name = "Outliers_features_PCA_3D"
plt.savefig(os.path.join(save_path, f"{save_name}.svg"), format='svg')
plt.show()

"""# 2- Feature Similarity/correlation"""

# Columns to examine:
X = dataset.iloc[:, 0:6] # Layer 1
# y = dataset.iloc[:, 6] # Label

save_path = FEATURE_CORR_PATH

"""## 2-1- Distribution"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

plt.rcParams.update({
    'figure.dpi': 300,
    'font.size': 10,
    'axes.titlesize': 12,
    'axes.labelsize': 11,
    'xtick.labelsize': 9,
    'ytick.labelsize': 9,
})

n_features = len(X.columns)
ncols = 3
nrows = int(np.ceil(n_features / ncols))

fig, axes = plt.subplots(nrows, ncols, figsize=(12, 4*nrows), constrained_layout=True)

for ax, col in zip(axes.flat, X.columns):
    data = X[col].dropna()
    # convert counts → percentages
    weights = np.ones_like(data) / len(data) * 100

    ax.hist(
        data,
        bins=10,
        weights=weights,
        color='tab:green',
        edgecolor='black',
        alpha=0.8
    )
    ax.set_title(col)
    ax.set_xlabel(col)
    ax.set_ylabel('Percentage (%)')
    ax.yaxis.set_major_formatter(PercentFormatter())       # show % ticks
    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)

# remove any empty subplots
for ax in axes.flat[n_features:]:
    fig.delaxes(ax)

fig.suptitle("Feature Distributions (n = {:,})".format(len(X)), fontsize=14)
plt.savefig(os.path.join(save_path, "features_distribution.svg"), bbox_inches='tight')
plt.show()

"""## 2-2- Feature Correlation"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def graph_lower_triangle_heatmap(corr_matrix, name, figsize=(8, 8), dpi=300):

    #Mask upper triangle
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)

    #scientific style
    sns.set_style("white")
    plt.rcParams.update({
        'font.family': 'serif',
        'font.serif': ['Times New Roman'],
        'font.size': 10,
        'axes.titlesize': 12,
        'axes.labelsize': 11,
        'xtick.labelsize': 9,
        'ytick.labelsize': 9,
        'figure.dpi': dpi
    })

    #figure & axes
    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)

    #Custom diverging palette
    cmap = sns.color_palette("vlag", as_cmap=True)

    #heatmap
    heatmap = sns.heatmap(
        corr_matrix,
        mask=mask,
        cmap=cmap,
        vmin=-1,
        vmax=1,
        center=0,
        annot=True,
        fmt=".2f",
        square=True,
        linewidths=0.5,
        cbar_kws={
            "shrink": 0.7,
            "label": "Pearson correlation coefficient",
            "ticks": [-1.0, -0.5, 0.0, 0.5, 1.0]
        },
        ax=ax
    )

    #Ticks and labels
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")
    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)
    ax.set_title(name, pad=15)
    fig.tight_layout()

    #Save
    fig.savefig(os.path.join(FEATURE_CORR_PATH, "Feature correlation matrix.svg"), dpi=dpi, bbox_inches='tight')

    plt.show()

X_pearson = X.corr(method='pearson')
graph_lower_triangle_heatmap(X_pearson, "Feature correlation matrix")

"""# 3- Feature selection"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_selection import SelectKBest, r_regression, f_regression, mutual_info_regression, RFE
from sklearn import linear_model
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LarsCV, LassoLarsCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.linear_model import BayesianRidge
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestRegressor

import shap

# Settings
sns.set_style("white")
plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'figure.dpi': 300,
    'font.size': 10,
})

Save_path = FEATURE_SELECTION_PATH

# assume dataset is a DataFrame
X_features = dataset.iloc[:, 0:6] # Layer 1
y = dataset.iloc[:, 6]
X_columns = X_features.columns

scalerX = sklearn.preprocessing.StandardScaler()
X_df = scalerX.fit_transform(X_features)
# X = X_df.values  # for methods that need arrays
X = X_df

# Univariate FS — returns Series index=feature, values in [0,1]
def compute_UFS(X, y):
    sel_r = SelectKBest(r_regression, k='all').fit(X, y)
    sel_f = SelectKBest(f_regression, k='all').fit(X, y)
    sel_m = SelectKBest(mutual_info_regression, k='all').fit(X, y)

    # raw scores / p‐values
    sr = np.abs(sel_r.scores_)
    sf = np.abs(sel_f.pvalues_)
    sm = np.abs(sel_m.scores_)

    # normalize each to [0,1]
    sr /= sr.max()
    sf /= sf.max()
    sm /= sm.max()

    # average the three metrics
    avg = (sr + sf + sm) / 3.0
    return pd.Series(avg, index=X_columns)

# RFE FS — returns normalized importance
def compute_RFES(X, y, k=None):
    # if k not given, select half
    k = k or X.shape[1]//2
    estimators = [
        ("Lasso",         linear_model.Lasso()),
        ("SGD",           linear_model.SGDRegressor()),
        ("SVR",           SVR(kernel="linear")),
        ("DecisionTree",  DecisionTreeRegressor()),
        ("BayesianRidge", BayesianRidge())
    ]
    scores = []
    for name, est in estimators:
        rfe = RFE(est, n_features_to_select=k).fit(X, y)
        imp = 1.0 / rfe.ranking_
        scores.append(imp / imp.max())

    mean_score = np.vstack(scores).mean(axis=0)
    return pd.Series(mean_score, index=X_columns)

# Coefficient‐based FS — returns normalized
def compute_FIC(X, y):
    models = [
        RidgeCV(alphas=np.logspace(-6,6,5)),
        LassoCV(alphas=np.logspace(-6,6,5)),
        ElasticNetCV(alphas=np.logspace(-6,6,5)),
        LarsCV(max_n_alphas=10000),
        LassoLarsCV(max_n_alphas=10000),
        SVR(kernel="linear")
    ]
    scores = []
    for m in models:
        m.fit(X, y)
        coef = np.abs(m.coef_).ravel()
        coef = coef / coef.max()
        scores.append(coef)
    mean_score = np.vstack(scores).mean(axis=0)
    return pd.Series(mean_score, index=X_columns)

# Permutation Importance — returns normalized
def compute_perm(X_df, y):
    rf = RandomForestRegressor(n_estimators=200, random_state=0).fit(X_df, y)
    perm = permutation_importance(rf, X_df, y, n_repeats=20, random_state=0)
    imp = perm.importances_mean
    imp = imp / imp.max()
    return pd.Series(imp, index=X_columns)

# SHAP Tree‐based Importances — returns normalized
def compute_shap(X_df, y):
    rf = RandomForestRegressor(n_estimators=200, random_state=0).fit(X_df, y)
    explainer = shap.TreeExplainer(rf)
    shap_vals = explainer.shap_values(X_df)
    imp = np.abs(shap_vals).mean(axis=0)
    imp = imp / imp.max()
    return pd.Series(imp, index=X_columns)

# -------------------------------------------
# Compute all importances
imp_UFS   = compute_UFS(X, y)
imp_RFES  = compute_RFES(X, y)
imp_FIC   = compute_FIC(X, y)
imp_perm  = compute_perm(X_df, y)
imp_shap  = compute_shap(X_df, y)

all_imps = pd.DataFrame({
    "Univariate feature selection":         imp_UFS,
    "Recursive Feature Elimination":        imp_RFES,
    "Feature importance from Coefficients": imp_FIC,
    "Permutation Feature Importance":       imp_perm,
    "SHAP":                                 imp_shap
})

# Unified bar plot
df_melt = all_imps.reset_index().melt(
    id_vars="index", var_name="Method", value_name="Importance"
).rename(columns={"index":"Feature"})
fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(
    data=df_melt,
    x="Feature", y="Importance", hue="Method",
    palette="tab10", edgecolor="black", ax=ax
)
ax.set_ylim(0, 1.1)

plt.xticks(rotation=45, ha="right")
plt.ylabel("Normalized importance")
plt.title("Feature Importance by Method")
plt.tight_layout()
plt.savefig(os.path.join(Save_path, "feature_importances_all_methods.svg"))
plt.show()

# Consensus plot
consensus = all_imps.drop(index='none', errors='ignore').mean(axis=1)
consensus = consensus.sort_values(ascending=False)

fig, ax = plt.subplots(figsize=(10, 6))
sns.barplot(
    x=consensus.index,   # features on the x-axis
    y=consensus.values,  # importance on the y-axis
    color='tab:green',
    edgecolor="black", ax=ax
)
ax.set_ylim(0, 1.1)
ax.set_yticks(np.linspace(0, 1, 6))

plt.xticks(rotation=45, ha="right")
plt.ylabel("Mean normalized importance")
plt.xlabel("Feature")
plt.title("Consensus Feature Importance")
plt.tight_layout()
plt.savefig(os.path.join(Save_path, "feature_importance_consensus_vertical.svg"))
plt.show()

"""# 4- Dataset clustering"""

# Columns to examine:
X = dataset.iloc[:, 0:6]
y = dataset.iloc[:, 6]

X_columns = X.columns

scaler = sklearn.preprocessing.StandardScaler()
X = scaler.fit_transform(X)

Save_path = CLUSTERING_PATH

"""## K-means 3D without volume"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

# style
sns.set_style("white")
plt.rcParams.update({
    'font.family':    'serif',
    'font.serif':     ['Times New Roman'],
    'mathtext.fontset':'stix',
    'axes.grid':      False,
    'figure.dpi':     300,
    'axes.titlesize': 14,
    'axes.labelsize': 12,
    'xtick.labelsize':10,
    'ytick.labelsize':10,
    'legend.fontsize':10
})

# Fit clustering on original data
clusterer = KMeans(n_clusters=3, random_state=0)
y_pred    = clusterer.fit_predict(X)

#  PCA down to 3 components
pca      = PCA(n_components=3)
X_pca   = pca.fit_transform(X)

# transform cluster centers into PCA space
centers_pca = pca.transform(clusterer.cluster_centers_)

# 3D scatter plot
fig = plt.figure(figsize=(8, 6))
ax  = fig.add_subplot(111, projection='3d')

palette = sns.color_palette("tab10", n_colors=3)
for cluster_id, color in enumerate(palette):
    mask = (y_pred == cluster_id)
    ax.scatter(
        X_pca[mask, 0],
        X_pca[mask, 1],
        X_pca[mask, 2],
        s=30,
        color=color,
        label=f"Cluster {cluster_id+1}",
        alpha=0.8,
        edgecolor='k',
        linewidth=0.5
    )

#
ax.set_title("K-means Clustering on PCA-Reduced Data", pad=15)
ax.set_xlabel("Principal Component 1", labelpad=10)
ax.set_ylabel("Principal Component 2", labelpad=10)
ax.set_zlabel("Principal Component 3")
ax.zaxis.set_rotate_label(False)
ax.zaxis.label.set_rotation_mode('anchor')
ax.zaxis.label.set_rotation(90)

ax.view_init(elev=25, azim=30)
ax.legend(loc='best', frameon=False)

fig.tight_layout()

#
save_plot(plt, "kmeans_pca3d_clustering_noVol", CLUSTERING_PATH)
plt.show()

"""## K-means 3D with volume"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.spatial import ConvexHull
from mpl_toolkits.mplot3d.art3d import Poly3DCollection

# style
sns.set_style("white")
plt.rcParams.update({
    'font.family':     'serif',
    'font.serif':      ['Times New Roman'],
    'mathtext.fontset':'stix',
    'axes.grid':       False,
    'figure.dpi':      300,
    'axes.titlesize':  14,
    'axes.labelsize':  12,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10
})

# Fit clustering
clusterer = KMeans(n_clusters=3, random_state=0)
y_pred    = clusterer.fit_predict(X)

# PCA down to 3 components
pca      = PCA(n_components=3)
X_pca   = pca.fit_transform(X)

#  3D scatter + transparent volumes
fig = plt.figure(figsize=(8, 6))
ax  = fig.add_subplot(111, projection='3d')

palette = sns.color_palette("tab10", n_colors=3)
for cluster_id, color in enumerate(palette):
    mask   = (y_pred == cluster_id)
    pts    = X_pca[mask]

    # scatter the points
    ax.scatter(
        pts[:, 0], pts[:, 1], pts[:, 2],
        s=30,
        color=color,
        label=f"Cluster {cluster_id+1}",
        alpha=0.8,
        edgecolor='k',
        linewidth=0
    )

    # draw convex hull if there are enough points
    if pts.shape[0] >= 4:
        hull = ConvexHull(pts)
        triangles = pts[hull.simplices]
        poly = Poly3DCollection(
            triangles,
            facecolor=color,
            edgecolor="none",
            alpha=0.2
        )
        ax.add_collection3d(poly)

#
ax.set_title("K-Means Clustering", pad=15)
ax.set_xlabel("Principal Component 1", labelpad=10)
ax.set_ylabel("Principal Component 2", labelpad=10)
ax.set_zlabel("Principal Component 3")
ax.zaxis.set_rotate_label(False)
ax.zaxis.label.set_rotation_mode('anchor')
ax.zaxis.label.set_rotation(90)

ax.view_init(elev=25, azim=30)
ax.legend(loc='best', frameon=False)

fig.tight_layout()

#
save_plot(plt, "kmeans_pca3d_clustering_WithVol", CLUSTERING_PATH)
plt.show()